# 深度学习 花书笔记

## 第一部分 应用数学与机器学习基础

### 数学基础

#### 第一章 引言

介绍深度学习的发展和历史

#### 第二章 线性代数

基础线性代数概念

实列 **PCA 主成分分析法** 保留数据尽可能多信息的降维方法

#### 第三章概率与信息论

概率分布 条件概率

概率分布的概念

#### 第四章 数据计算

上溢和下溢

梯度的优化法（梯度下降）
$$
x' = x-\epsilon \nabla_xf(x) \\其中 \epsilon  为学习率
$$

实例 **线性最小二乘**

### 第五章 机器学习基础

机器学习解决的问题 分类 回归 异常检测 合成和采样 缺失值填补 去噪 密度估计

无监督学习： 密度估计、聚类

监督学习: 样本有标签(label)或者目标(target)

线性回归： 学习一个线性模型，例如一元线性回归方程为：
$$
y^, = W^Tx+b
$$
 正则化： 修改学习算法，使其降低泛化误差而非训练误差。	如加入**权重衰退**

引用验证集是为了训练超参数。

当数据集太小可以使用交叉验证，有K-折交叉验证、留一法 、LOSO交叉验证

支持向量机SVM(二分类)：
$$
y=\begin{cases} 
1 \,, w^tx+b>=0 \\
0 \,, w^tx+b< 0
\end{cases}
$$


随机梯度下降 SGD 梯度是期望， 期望可用于小规模的样本近似估计。

### 第一部分总结

第二章到第三章重点讲了关于深度学习所用的数学基础，包括线代，概率论，偏导数（梯度）等等。

第五章重点讲了机器学习的基础，和一些重要方法，如线性回归、支持向量机等。并介绍一些机器学习的概念的如超参数，验证集。把机器学习算法分为监督学习和无监督学习两种。介绍了限制传统机器学习泛化能力的因素，如维数灾难（一组变量不同的配置数量会随着变量数目的增加指数级增长）

局部不变性和平滑正则化。



## 第六章 深度前馈网络

### 输出单元

#### 1.用于高斯输出分布的线性单元

线性单元是一个简单的神经元模型，其输出是输入的线性组合，通常表示为：
$$
y=wx+b
$$


其中，\(y\) 是线性单元的输出，\(x\) 是输入，\(w\) 是权重，\(b\) 是偏置线性单元的输出是输入的线性变换，没有非线性激活函数。在高斯输出分布中，线性单元的输出可以用于表示线性关系，适用于回归任务或其他需要建模线性关系的场景。

#### 2.用于Bernouli分布输出的分布的sigmoid单元

用于Bernoulli分布输出的分布的sigmoid单元是一种常见的模型组件，通常用于二元分类问题。这个单元将输入转换为一个介于0和1之间的概率值，表示某个事件发生的概率。

在神经网络中，sigmoid单元的数学表达式如下：
$$
f(x) = \frac{1}{1 + e^{-x}}
$$
其中，\(f(x)\) 是输出的概率值，\(x\) 是输入。

在二元分类问题中，通常使用sigmoid单元作为输出层的激活函数。模型的输出将被解释为某一类的概率（例如，正类别的概率），然后可以根据一个阈值来进行分类决策。如果输出概率大于等于阈值，模型预测为正类别；否则，模型预测为负类别。

sigmoid单元常用于逻辑回归模型和神经网络中的二元分类问题，因为它能够产生0到1之间的输出，适用于伯努利分布（二项分布）的建模。

#### 3.用于Multinoulli输出分布的softmax单元

Softmax单元是一种用于多类别分类的神经元，通常用于输出分布概率的归一化。在这里，假设有多个类别（例如，分类任务中的不同类别），Softmax单元将输入进行归一化，使得每个类别的输出是一个介于0和1之间的概率值，并且所有类别的输出概率之和为1。

Softmax函数的数学表达式如下：

$$
P(y_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}
$$

其中，$P(y_i)$ 表示第 $i$ 个类别的输出概率，$z_i$ 是输入到Softmax单元的值，$K$ 是类别的总数。

Softmax单元可以将一个向量作为输入，并输出一个概率分布，其中每个元素表示相应类别的概率。这使得Softmax单元非常适合多类别分类任务，例如图像分类或文本分类，其中需要将输入分为多个不同的类别。

### 隐藏单元

#### **整流线性单元（Rectified Linear Unit，ReLU）**

ReLU是隐藏单元的极好的默认选择。

它使用激活函数$ g(z)=max{(0,z)}$ .

ReLU激活函数的优点包括：

1. **非线性性质**：尽管ReLU是一个线性函数（z当z大于等于零时），但整体激活函数是非线性的，这对于神经网络来说很重要，因为它使网络能够学习复杂的非线性关系。

2. **稀疏激活性**：在训练过程中，ReLU网络通常会产生稀疏的激活，因为它将负值部分截断为零。这可以节省计算资源并减少过拟合的风险。

3. **缓解梯度消失问题**：相对于一些传统的激活函数（如sigmoid和tanh），ReLU对梯度消失问题的影响较小，因为它在正区间上有一个恒定的梯度（1）。

4. **计算效率**：ReLU的计算非常高效，因为它仅涉及简单的比较和阈值设置操作。

尽管ReLU有许多优点，但它也存在一些问题，例如：

1. **神经元死亡问题**：当神经元在训练过程中被激活为零后，可能会一直保持不活跃，这被称为神经元死亡问题。这可以通过使用ReLU的变种（如Leaky ReLU或Parametric ReLU）来缓解。

2. **不适合所有情况**：ReLU可能不适合所有类型的数据和任务。对于某些问题，特别是处理负数输入的情况，其他激活函数（如tanh或sigmoid）可能更合适。

#### **Logistic Sigmoid函数**：

Logistic Sigmoid函数的数学表达式：
$$
sigma(z) = \frac{1}{1 + e^{-z}}
$$


其中，σ(z)是Sigmoid函数的输出，z是输入。Sigmoid函数的特点是它将输入映射到0到1之间的范围，它的输出值表示了一个事件发生的概率。Sigmoid函数通常用于二元分类问题，其中网络需要输出一个概率来表示样本属于某个类别的可能性。然而，Sigmoid函数的缺点之一是它在输入非常大或非常小的情况下会饱和，导致梯度消失问题，这在深度网络中可能会成为问题。

#### **双曲正切函数**：

双曲正切函数的数学表达式如下：
$$
\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}
$$
与Sigmoid函数类似，tanh函数将输入映射到-1到1之间的范围。不同之处在于tanh函数的输出是零中心的，即它的均值接近零，这可以帮助减少梯度消失问题，相对于Sigmoid函数在零点附近的梯度更陡峭。tanh函数通常用于隐藏层中，尤其是在循环神经网络（RNN）中，因为它对于处理序列数据的中心化很有帮助。

#### 隐藏单元总结

需要注意的是，虽然Sigmoid和tanh函数在某些情况下仍然有用，但在深度学习领域，ReLU（Rectified Linear Unit）及其变体已经成为更常见的选择，因为它们在训练中更容易收敛，同时避免了一些梯度问题。选择激活函数通常取决于特定任务和网络结构的要求。

Logistic Sigmoid函数和双曲正切（Hyperbolic Tangent）函数都是常用的激活函数，通常用于神经网络的隐藏层或输出层。它们都具有非线性特性，允许神经网络学习复杂的非线性关系。

### 反向传播（BP算法）

基本步骤：

1. 正向传播（Forward Propagation）：首先，将输入数据传递给神经网络，通过网络的前向传播过程计算出预测值。
2. 计算损失：将预测值与实际标签进行比较，计算损失函数的值。
3. 反向传播误差（Backpropagate Error）：从输出层开始，计算损失函数关于每个参数的梯度。这是通过链式法则（Chain Rule）来实现的，将梯度从输出层向输入层传播，同时在每一层更新权重和偏差。
4. 更新参数（Update Parameters）：利用计算得到的梯度信息，使用优化算法（如随机梯度下降或Adam）来更新神经网络的参数，减小损失函数的值。
5. 重复迭代（Iterate）

## 第七章 深度学习中的正则化

### **参数范数惩罚**

它通过添加一个额外的项到模型的损失函数中，从而限制了模型的学习能力。

### **L2正则化**（岭回归）

正则化是对学习算法的修改，旨在减少泛化误差而不是训练误差。

![image-20230927223333569](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20230927223333569.png)

在线性回归的基础上加入正则项 后，能够避免过拟合。其本质是通过权重衰减，弱化不显著的特征所占权重。

目标函数为两个正数项相加，因此在优化过程中，两个正数项都要逐渐趋近于0。可知，**该正则项实际的作用是希望权重越小越好。也就是说，希望忽略一些不重要的特征**。

使用L2范数的一个原因是它对权重向量的大分量施加了巨大的惩罚。 这使得我们的学习算法偏向于在大量特征上均匀分布权重的模型。 在实践中，这可能使它们对单个变量中的观测误差更为稳定。对权重$ W$进行惩罚，从程度取决于$a$。

### **范数**

![image-20230928151506058](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20230928151506058.png)

L1正则化是指权值向量$w$中各个元素的绝对值之和，通常表示为$\|\mathbf{x}\|_1 = \sum_{i=1}^n \left|x_i \right|.$

L2正则化是指权值向量$w$中各个元素的平方和然后再求平方根（可以看到Ridge回归的L2正则化项有平方符号），通常表示为$\|\mathbf{x}\|_2 = \sqrt{\sum_{i=1}^n x_i^2}$

$L_p$范数 $\|\mathbf{x}\|_p = \left(\sum_{i=1}^n \left|x_i \right|^p \right)^{1/p}.$

L1正则化可以产生稀疏权值矩阵（即，最优值中的参数变为零），产生一个稀疏模型，从而实现特征选择和降维的效果。
L2正则化可以防止模型过拟合（overfitting）；一定程度上，L1也可以防止过拟合

### 数据增强

图像识别中可以平移图像，添加噪声，旋转，色调偏移等等

望模型能够在这些变换或干扰不受影响保持预测的准确性，从而减小泛化误差。

### 标签平滑

在常见的多分类问题中，先经过softmax处理后进行交叉熵计算，原理很简单可以将计算loss理解为，为了使得网络对测试集预测的概率分布和其真实分布接近，常用的做法是使用one-hot对真实标签进行编码，然后用预测概率去拟合one-hot的真实概率。但是这样会带来两个问题：

无法保证模型的泛化能力，使网络过于自信会导致过拟合；
全概率和0概率鼓励所属类别和其他类别之间的差距尽可能加大，而由梯度有界可知，这种情况很难adapt。会造成模型过于相信预测的类别。

标签平滑可以解决这两个问题，他的的过程如下：

1. 对于每个训练样本的真实标签，将其从 1 减去一个较小的值 ε，同时将这个 ε 值分配给其他类别。这个 ε 值通常很小，比如 0.1 或更小。
2. 调整后的标签需要重新归一化，以确保它们的总和仍然等于 1，以保持标签的概率性质。

### 多任务学习

多任务学习也是希望令模型的参数能够进行很好的泛化，其原理是对多个目标共享模型的一部分（输入及某些中间的表示层），使其对于多个有关联的目标均有较好的效果，保证模型可以更好的推广。

### Bagging和集成学习

集成方法的核心思想是通过结合多个模型的预测来减少单个模型的误差，从而提高整体的泛化能力。

1. **Bagging（Bootstrap Aggregating）**：Bagging通过随机选择训练数据的子集，训练多个相同类型的模型（如决策树），然后将它们的预测结果进行平均或投票来减少方差，提高模型的稳定性。随机森林就是一种基于Bagging的集成方法。
2. **Boosting**：Boosting通过反复训练一系列的弱学习器，每次都关注之前模型错误分类的样本，从而逐渐提高模型性能。常见的Boosting算法包括AdaBoost、Gradient Boosting和XGBoost等。
3. **Stacking**：Stacking是一种将不同类型的基本模型（如决策树、支持向量机、神经网络等）与一个元模型结合的方法。基本模型用于生成预测，而元模型则用这些基本模型的预测结果作为输入来进行最终的预测。
4. **Voting**：Voting是将多个独立训练的模型的预测结果进行投票或平均来做出最终的预测。有硬投票（多数票决定）和软投票（加权平均）两种形式。

### Dropout

Dropout是一种正则化技术。在训练神经网络时，Dropout会随机丢弃一些神经元的输出，使得模型不会过度依赖于特定的神经元，从而减少过拟合的风险。这种随机丢弃操作只在训练期间，在测试期间所有神经元都会参与，以产生最终的预测结果。

## 第八章 深度模型中的优化

### 训练模型时的主要挑战

1.优化凸函数时Hessian矩阵H的病态

2.局部最小值

3.鞍点

4.梯度爆炸

###  随机梯度下降 SGD

![image-20231007190317575](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231007190317575.png)



### 使用动量的SGD

通过引入动量项来在加速收敛并减少训练过程中的震荡，动量项考虑了之前梯度更新的方向，并在当前更新中添加一个动量成分，有助于克服梯度下降中的局部最小值问题。

![image-20231007190559734](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231007190559734.png)

​             

瞬时速度 = 动量 * 瞬时速度 - 学习率 * 梯度
新参数 = 旧参数 + **瞬时速度**

### AdaGrad

与Momentum方法比较，AdaGrad没有引入速度变量 $v$ ，而是记录每个参数方向上的梯度的值的平方和，在该参数方向上步进时除以这个平方和的平方根，则对于原梯度较小学习进展较慢的方向相较于原梯度较大的方向rescale的程度较小，从而加速在该方向上的学习进程.

![image-20231007192610166](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231007192610166.png)

### Adam

Adam是一种学习率自适应的优化算法，它结合了动量梯度下降和自适应学习率的特点。

![image-20231007195629516](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231007195629516.png)

## 第九章 卷积网络

CNN通常适用于具有网格状结构的数据，例如时序数据可看做是在特定时间间隔上的一维网格，图像可以看做是像素构成的二维网格，医学成像如CT等为三维网格数据。

卷积是一种特殊的线性运算。

### 卷积运算

![image-20231007204407956](C:\Users\chen\AppData\Roaming\Typora\typora-user-images\image-20231007204407956.png)

### CNN的优点？

1.稀疏连接

在CNN中，卷积层使用卷积核来提取特征。每个卷积核仅与输入数据的感受野）相连接，而不是与整个输入相连接。这种局部连接意味着每个神经元仅对输入的一部分信息感兴趣，从而实现了稀疏连接。

2.参数共享

参数允许在整个输入空间中重复使用相同的卷积核，以便检测相似的特征模式。在卷积层中，卷积核被应用于输入数据的不同位置，但卷积核的参数是共享的，即卷积核在不同位置检测相同的特征。

3.等变表示

1. 平移等变性（Translation Equivariance）：对于图像处理任务，输入图像中的特征在图像上的平移变换应该反映在网络输出中的相应平移变换。这意味着如果在输入图像中移动物体，CNN应该能够检测到物体在输出中的相应移动，而不是重新学习相同的特征。
2. 尺度等变性（Scale Equivariance）：尺度等变性是指如果输入数据的尺度发生变化，神经网络的输出应该以相应的尺度变换来响应。如果输入是一个物体的图像，该物体可以以不同的尺度出现在图像中，网络应该能够在不同尺度上检测到相同的特征。

**Invariance（不变性）：F(X) = F[Shift(X)]**

**Equivariance（等变性）：Shift[F(X)] = F[Shift(X)]**

### 池化层 pooling

1.最大池化 2.平均池化

池化的作用

（1）保留主要特征的同时减少参数和计算量，防止过拟合。

（2）invariance(不变性)，这种不变性包括translation(平移)，rotation(旋转)，scale(尺度)。

Pooling 层说到底还是一个特征选择，信息过滤的过程。也就是说我们损失了一部分信息，现在有些网络都开始少用或者不用pooling层了。

## 第十章 序列建模：循环和递归网络

### 循环神经网络

